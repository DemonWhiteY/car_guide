

# 开发背景

​        随着智能汽车和自动驾驶技术的快速发展，车载人机交互系统正经历从单一物理按键向多模态融合交互的范式转变。传统车载交互系统依赖触控屏与物理旋钮的交互方式，在驾驶场景中存在注意力分散、操作效率低下等问题。例如，触摸屏在驾驶过程中可能分散驾驶员注意力，增加操作风险；而语音指令在嘈杂环境下识别准确率会降低。因此，开发车载多模态交互系统成为当务之急。该系统旨在整合多种交互方式，如语音、手势、触摸、眼神等，通过多模态融合技术，使驾驶员能够以更自然、便捷、安全的方式与车辆进行交互。这不仅能够提升驾驶体验，还能有效降低驾驶风险，满足现代驾驶者对于高效、智能、安全驾驶环境的迫切需求，同时也顺应了汽车智能化、人性化的发展趋势。本软件基于windows平台开发了面向智能座舱的多模态交互系统原型，整合手势识别（动态手势/静态手势）、面部表情识别（视线追踪/唇动检测）和语音交互三维度通道，通过多源信息融合算法实现意图理解的协同互补，初步验证了多模态交互在提升驾驶安全性与交互自然度方面的技术可行性，为后续车载嵌入式部署及多模态数据同步优化奠定技术基础。

# 项目目标

**一、交互体验提升目标**

  1. **多模态融合精准度**

     ​	实现语音、手势、触摸、眼神等多种交互模态的高精度融合。例如，在嘈杂环境下，系统能够准确区分驾驶员的语音指令和背景噪音，同时结合手势动作来进一步确认指令意图。对于手势识别，要达到能够精准识别驾驶员手指细微动作的程度，如精确到毫米级的手指位移和角度变化，确保手势指令的准确执行。

  2. **自然流畅的人机交互**

     * 构建一个能够理解自然语言的语音交互系统。例如，当驾驶员说 “我觉得有点热” 时，系统能够自动理解并调整车内温度，而不是需要驾驶员说出具体的 “将温度调低 2 度” 这样的机械指令。
     * 开发直观、易用的手势交互界面。手势交互方面，设计一套符合人体工程学和驾驶习惯的手势库，让驾驶员无需记忆复杂的手势组合就能完成如调节音量、切换歌曲、接听电话等常见操作。

  3. **个性化交互定制**

     ​	根据驾驶员的实时状态和偏好动态调整交互方式。例如，如果系统检测到驾驶员处于疲劳状态，可以自动切换到更简洁、直接的交互模式，减少信息呈现量，同时通过语音提醒驾驶员注意休息。

**二、驾驶安全增强目标**

  1. **分心驾驶干预**
     * 利用多模态交互系统实时监测驾驶员的注意力状态。通过模拟车内摄像头和传感器结合，分析驾驶员的头部朝向、眼神聚焦点、操作频率等多模态数据。当检测到驾驶员出现分心行为，如东张西望等，系统能够及时发出警报，并采取相应的干预措施。例如，发出语音提醒。
     * 优化交互系统对紧急情况的响应机制。在模拟遇到突发状况，如前方车辆突然刹车、道路出现障碍物等，系统能够迅速通过多种交互方式提醒驾驶员。例如，同时发出急促的语音警报、闪烁警示图标，使驾驶员能够快速做出反应。

**三、系统性能优化目标**

  1. **高精度识别与低误判率**
     * 对于语音识别，要达到在各种环境噪声条件下（如高速行驶时的风噪、发动机噪音等）语音识别准确率较高。同时，不断优化语音识别算法，降低因方言、口音等因素导致的误判率。
     * 在手势识别方面，确保在不同光照条件（如强光直射、夜晚微光环境）下手势识别的准确率均能达到 90% 以上。

  3. **稳定性与可靠性**
     
     ​	通过严格的软件测试，进行充分的代码优化和容错设计，防止因软件故障导致交互系统失效，确保多模态交互系统在各种复杂的驾驶场景下稳定运行。
     
     # 开发环境
     
     我这里是python 3.10
     
     requirements:
     
     cv2
     
     mediapipe
     
     windows
     
     # 系统设计
     
     ## 3.1 整体架构
     系统采用分层模块化设计，分为四层：  
     
     1. **输入层**：通过摄像头或视频流捕获手部动作，借助MediaPipe Hands模型实时提取21个手部关键点3D坐标。  
     2. **处理层**：  
        - *手势检测*：基于关键点空间关系定义阈值逻辑（如指尖与指节相对高度），通过设定手指屈伸判断规则以及特定手势组合逻辑，实现静态手势分类，如 “yes”“tick”“fist”“thumbs_up”“wave” 等，若无法匹配则归为 “unknown”。。  
        - *时序分析*：通过连续帧投票机制（`gesture_counter`统计）提升动态手势（挥手）识别鲁棒性。
     3. **决策层**：载预先存储的手势与指令码对应关系的数据库，将识别出的手势映射到相应的指令码上，实现手势与指令的关联，以便后续根据手势执行对应的操作。
     4. **控制层**：支持动态配置（`change`命令），允许通过视频样本添加/修改/删除指令映射，实现闭环迭代优化。  
     
     ## 3.2 关键特性
     
     - **轻量化推理**：依赖MediaPipe预训练模型，CPU实时处理，识别迅速，方便本地部署；  
     - **规则融合学习**：混合基于物理特征的硬规则（`classify_gesture`）与数据驱动的软决策（预留模型扩展接口）；  
     
     # 详细设计
     
     ## 4.1 手势识别模块
     
     - **特征提取**：  
       
       ```python
       def classify_gesture(landmarks):
           # 计算手指开合状态：通过指尖(tip)与指节(pip)的Y坐标比较
           tip_ids = [4, 8, 12, 16, 20]
           pip_ids = [3, 6, 10, 14, 18]
           fingers = [landmarks[tip].y < landmarks[pip].y for tip, pip in zip(tip_ids[1:], pip_ids[1:])]
       ```
       
     - **多条件决策树**：  
       - **手指状态判断** ：对于除大拇指外的其他四指，通过比较其指尖关键点（tip_ids）与中间指关节关键点（pip_ids）在 y 坐标上的大小关系，来判断手指是伸展状态还是弯曲状态。若指尖 y 坐标小于中间指关节 y 坐标，则认为该手指伸展，反之则弯曲。同时，针对大拇指的伸展状态判断，不仅考虑其指尖与近端指节（ip）在 y 坐标上的位置关系，还需结合其与掌根关键点在 x 坐标上的距离差异，综合判断大拇指的伸展情况。
       
       - **手势分类逻辑** ：根据手指的伸展状态组合以及特定的关键点位置关系，定义不同的手势类型。例如，当食指和中指伸展且其他手指弯曲时，判断为 “yes” 手势；当食指伸展且大拇指张开一定角度时，判断为 “tick” 手势；当所有手指都弯曲且大拇指也未张开时，判断为 “fist” 手势；当大拇指伸展且其他手指弯曲数量不超过 1 个时，判断为 “thumbs_up” 手势；当有 4 个及以上手指伸展时，判断为 “wave” 手势。
       
       - 预留`unknown`类别处理未定义手势，避免误触发，同时为日后拓展留下接口。
       
         ```python
         index_up = landmarks[8].y < landmarks[6].y
         middle_up = landmarks[12].y < landmarks[10].y
         ring_up = landmarks[16].y < landmarks[14].y
         pinky_up = landmarks[20].y < landmarks[18].y
         
         thumb_up_y = landmarks[4].y < landmarks[3].y  # 指尖高于 IP
         thumb_up_x = abs(landmarks[4].x - landmarks[5].x) > 0.05  # 与掌根 X 差异较大
         thumb_open = thumb_up_y and thumb_up_x
         
         # 逻辑判断
         if index_up and middle_up and not ring_up and not pinky_up:
             return "yes"
         if index_up and thumb_open and not middle_up and not ring_up and not pinky_up:
             return "tick"
         if open_count == 0 and not thumb_open:
             return "fist"
         if thumb_open and open_count <= 1:
             return "thumbs_up"
         if open_count >= 4:
             return "wave"
         
         return "unknown"
         ```
       
           
     
     ## 4.2 指令管理模块
     
     - **数据库交互**：  
       - **数据库加载** ：在程序启动时，调用 load_gesture_db 函数，检查手势数据库文件是否存在，若存在则逐行读取文件内容，将手势名称和对应的指令码以键值对的形式存储到一个字典对象中，以便后续快速查询和使用。
       - **手势识别与指令获取** ：在普通的手势识别模式下，调用 recognize_gesture_from_video 函数对视频进行手势识别，得到识别出的手势名称后，利用加载的数据库字典，通过手势名称作为键值检索对应的指令码，若存在则输出该指令码，否则输出未识别到有效指令的提示信息。  
       - **冲突检测机制**：添加新映射时校验手势唯一性（`if gesture in db`）。  
       - **添加手势与指令关联** ：当用户输入以 “change” 开头且操作类型为 “a” 的指令时，调用 recognize_gesture_from_video 函数对指定视频进行手势识别，若识别出有效手势且该手势不在当前数据库中，则将该手势与指定的指令码添加到数据库字典中，并调用 save_gesture_db 函数将更新后的数据库保存回文件。
       - **删除手势与指令关联** ：若用户输入的操作类型为 “d”，则根据指定的指令码，在数据库字典中查找所有与该指令码关联的手势，并将其一一删除，最后保存更新后的数据库。
       - **替换指令码** ：对于操作类型为 “r” 的情况，先对指定视频进行手势识别，若识别出有效手势且该手势不在数据库中，则检查当前数据库中是否存在使用目标指令码的手势，若存在则删除旧手势与该指令码的关联，再将新识别的手势与该指令码添加到数据库中并保存。
       
       ```python
       def handle_command(command_str, file_path):
           # 支持三种操作：替换(r)、添加(a)、删除(d)
           if action == 'r':  # 替换现有指令
               old_gestures = [gesture for gesture, code in db.items() if code == ncode]
               del db[old_gesture]  # 删除旧手势绑定
       ```
     
     ---
     
     # 五、数据库设计
     ## 5.1 结构设计
     
     - **实体表**：`gesture_command`  
       
       | 字段名       | 类型    | 描述         | 示例        |
       | ------------ | ------- | ------------ | ----------- |
       | gesture_name | VARCHAR | 手势唯一标识 | "thumbs_up" |
       | command_code | CHAR(3) | 3位指令编码  | "001"       |
       
     - **存储形式**：纯文本文件（`gesture_db.txt`），每行记录格式为`gesture,code`，通过逗号分隔。  
     
     ## 5.2 操作接口
     
     - **加载数据库** ：通过读取数据库文本文件，逐行解析手势名称和指令码，并存储到内存中的字典对象里，以便程序快速访问和操作手势数据。若文件不存在，则初始化一个空字典作为数据库。
     - **保存数据库** ：当数据库内容发生更新，如添加、删除或修改手势与指令码的关联关系时，将内存中的字典数据重新写入到数据库文本文件中，覆盖原有内容，确保数据库文件与内存中的数据保持一致。
     - **查询操作** ：在手势识别过程中，以手势名称为键值，在内存中的数据库字典里快速检索对应的指令码，查询时间复杂度为 O(1)，能够高效地实现手势到指令码的映射。
     - **更新操作** ：对于添加新手势与指令关联的情况，直接在字典中添加新的键值对；删除操作则是根据指令码或手势名称，从字典中移除对应的键值对；替换指令码操作涉及到先删除旧手势与指令码的关联，再添加新手势与目标指令码的关联，所有更新操作完成后同步保存到数据库文件中。
     
     # 系统测试
     
     ## 测试环境
     
     - python：python 3.10
     
     - 操作系统：windows 11
     
     - CPU: intel i9 12900H laptop
     
     - GPU:NVIDIA GEFORCE 3070Ti laptop
     
     - 测试数据集：采用自行录制的手势数据集，数据集格式为mp4视频，长度10s以内，采用纯色背景。
     
     ## 功能测试
     
     ​	采用自行录制的数据集，于主函数致中调用进行识别，识别结果均正确且指令修改操作也可正常执行，准确率为100%，具体执行如下：
     ![image-20250526205438541](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250526205438541.png)
     
     ## 性能测试
     
     ​	采用cprofile进行性能测试，主要测试返回时间，以适应车载场景，具体测试结果如下：
     
     ![屏幕截图 2025-05-26 225300](E:\软件工程\gesture\屏幕截图 2025-05-26 225300.png)
     
     ![performance_comparison](E:\软件工程\gesture\performance_comparison.png)

可见对于较短的视频，模块均可较为迅速的处理，及时反馈。

# 用户手册

本模块支持5种手势，返回对应的opcode指令码，详细对应关系可以查看gesture_db.txt，支持对手势的增删改，指令格式为change opcode r/a/d （对应增删改），以手动录制对应的要求数据集，测试通过。在主函数中调用，参数分别为要识别的视频路径，是否是要进行数据库修改，如为false之后参数可以忽略，之后为指令，最后一个为如果需要增加或者替代，则需要提供对应视频路径。调用之后即时返回。